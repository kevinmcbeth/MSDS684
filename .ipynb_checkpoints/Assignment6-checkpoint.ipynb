{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning and TD learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the last discussion, we covered the Monte Carlo algorithm. The Monte Carlo algorithm is a simple mean reward update for each state seen in an episode. In the Q learning algorithm and the TD learning algorithm, the a movement from a state to another state causes an update in the beginning state. TD learning is regarded as on-policy because the update to the value of the state depends on the next state. Q learning is regarded as off-policy because the update to the value of the state depends on the maximum reward and value transition for all possible actions from that state. This leads to Q learning having the benefit of exploration without causing a detrimental impact to it's overall learning rate.\n",
    "\n",
    "In this lab we will explore the pros and cons of each approach. The first discussion has to do with memory consumption of the algorithms. The second discussion has to do with learning rate, or total iterations necessary to solve the problem. This second discussion is accompanied by a count experiment with both TD learning and Q learning, measuring the number of times the internal loops run prior to the largest change in an iteration being smaller than 10^-3. Alphas were changed to 0.2 and gammas to 0.9.\n",
    "\n",
    "### Results\n",
    "\n",
    "Count Iterations:\n",
    "1) Q learning - 2747, 2661, 2147, 2925, 3272\n",
    "2) TD learning - 101360, 2548, 86246, Did not converge in 100k iterations, 46093\n",
    "\n",
    "### Discussion\n",
    "The major issue with Q-learning is the necessity to store an element for each state action pair. This memory complexity is much more problematic than using TD learning. In the grid world we've been using throughout the course, this leads to an extra memory consumption of 44 floats, one for each state action pair. Meanwhile, the value function only stores 12 floats, leading to a memory usage of around 25%. This cost of memory is insignificant for this problem, but can be a major inhibitor for larger sets of states and actions. Python utilizes 4 byte floats for the problems we've been exploring. On an 16 gb memory stick, or 2^34 bytes, one could fit in 2^32 states for the TD-0 method. If the action space was 4, one could only account for 2^30 states. As action states grow, the allowable number of states shrinks proportionally.\n",
    "\n",
    "The benefit of Q-learning compared to TD learning is shown in the results section. The TD-0 learning algorithm was inconsistent. It actually has a more powerful version known as TD-lambda, which has a discounted look ahead (like gamma), but the naive look ahead was prone to falling into non productive updates. Q-learning was very consistent, and converged in much fewer inner loop operations than TD learning\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this discussion we explored Q learning and TD learning, specifically looking at the memory requirements of the algorithms and time to conervgence for the algorithms. Q learning requires a multiplicative of the average amount of actions per state for memory but converges in a significantly faster time. TD learning requires less memory but is prone to inconsistent convergence times.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
