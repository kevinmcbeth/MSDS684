{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Assignment, Exploring The Update Rule\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This week's assignment explores the usage of a simple value function updating reinforcement learning algorithm for teaching a computer to play tic-tac-toe. Essentially this is done by calculating a value for each state, by playing multiple games of tic tac toe between computers. At the conclusion of each game, the values are backtracked for each state based on the final reward. This iteratively changes the value for seen states.\n",
    "\n",
    "This seen states distinction is quite important. My code has adjusted the tic tac toe players to alternate between who goes first and who goes second. The original code had player 1 always start and player 2 always play second. This explains why individuals who played the player 1 second would win, as the player 1 computer would have never seen the states to be played against, defaulting into choosing an essentially random algorithm.\n",
    "\n",
    "In this exploration of the code, I will change the learning rates and play the computer after 500 iterations of training. The specific metric to be used is to play the computer 3 times for each learning rate alternating who goes first and document the results of win, loss, or draw. My expectation is that a medium learning rate will achieve the best training, as a high alpha might impact a given state too far, and a low learning rate will slow the training, resulting in the algorithm taking longer to study. I will use 0.001, 0.1, 0.5 and 0.9 for the test alphas.\n",
    "\n",
    "The second test will be to change the epsilon. I expect a medium epsilon to achieve the best results, as an epsilon too low will prevent proper exploration of a complex problem and an epsilon too high will result in the opponent making too many explorative moves instead of exploitative moves during gameplay. I will use epsilons of 0.01, 0.1, 0.2, and 0.8 to show this, at a fixed alpha for the best achieving alpha in the previous test. I will follow the same procedure of training length at 500 games and a test play through of 3 games.\n",
    "\n",
    "## Results\n",
    "\n",
    "The results of the experiment were as follows. \n",
    "\n",
    "Changing Alpha Results (Wins, Losses, Draws)\n",
    "* 0.001 : 3-0-0\n",
    "* 0.1 : 1-1-1\n",
    "* 0.5 : 1-0-2\n",
    "* 0.9 : 3-0-0\n",
    "\n",
    "As one can see, alphas between 0.1 and 0.5 performed much better in the limited sample size of games against a human. This follows along with the intuition that a slow learning update rate negatively impacts a players learning ability over the short term and that a fast learning update rate negatively impacts a players scope based on overfitting early training models.\n",
    "\n",
    "The test results for changing epsilon at a fixed alpha of 0.1 is as follows:\n",
    "\n",
    "Changing Epsilon Results (Wins, Losses, Draws)\n",
    "* 0.01 : 1-0-2\n",
    "* 0.1 : 1-0-2\n",
    "* 0.2 : 3-0-0\n",
    "* 0.8 : 2-0-1\n",
    "\n",
    "As one can see, a low epsilon rate didn't have a negative impact on model performance. The value functions for all epsilons would have played well if taking the greedy choice. The 0.2 algorithm was unfortunately choosing to take a random choice too often, as was the 0.8 epsilon agent.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This experiment briefly tested the Tic-tac-toe value function updating algorithm. We tested changing the alpha at a fixed epsilon, and changing the epsilon at a fixed alpha. As we expected, moderate learning rates were better for the agents performance as it encouraged slow but not too slow updates. A low epsilon was better for the performing model because it made more optimal decisions when playing the human instead of experimenting during the test set.\n",
    "\n",
    "Other things which an individual could try for this experiment would be to:\n",
    "* Explore having a high epsilon for training before moving to a lower epsilon for testing.\n",
    "* Using epsilon decay for the algorithm\n",
    "* Having a better value function metric for calculating the performance of the algorithms. For instance, taking a fixed set of 20 positions which there is an obvious move the computer should take to win or draw and testing whether or not the computer makes that move.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
