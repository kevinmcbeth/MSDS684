{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration Policy Iteration\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the last assignment, we looked at policy evaluation and policy iteration. In this assignment, we explore value iteration as compared to policy iteration. Policy iteration finds a better policy given the value function calculated for the previous policy evaluation. Value iteration ignores a given policy and instead updates the value function to find the maximum value function. It in some semblance is similar to iterative policy evaluation except that it doesn't operate for a given policy, but instead maximizes the value function based upon the possible actions and rewards from a given state.\n",
    "\n",
    "The easiest way to compare the two methods is to estimate the run time for a simple problem. To do this, I put in count incrementers in various locations. For the policy iteration method, I inserted counts inside of both the evaluation and the iteration steps inside the if s in policy lines. For the value iteration I inserted a count inside of the actions iterators for each state. My hypothesis is that the value iteration steps will take vastly fewer iterations. This is mainly due to the expense of the iterative policy evaluation step, which more or less recalculates the value function for each policy. The value function is similar to a single policy evaluation step. For the negative grid, I would expect the number of counts for iterative policy evaluation divided by the number of policy iterations should roughly equal the number of counts for value iteration.\n",
    "\n",
    "To test this I will use the negative grid with standard parameters. In policy iteration there are three counters. count is for the number of policy evaluation steps, count2 for the number of policy iteration steps, and policyCount if for the number of iterations. For value iteration, count is the number of value iteration steps.\n",
    "\n",
    "### Results\n",
    "\n",
    "Trial 1:\n",
    "Policy Iteration - 513, 27, 3\n",
    "Value Iteration - 180\n",
    "\n",
    "Trial 2:\n",
    "Policy Iteration - 558, 54, 6\n",
    "Value Iteration - 144\n",
    "\n",
    "Trial 3:\n",
    "Policy Iteration - 540, 36, 4\n",
    "Value Iteration - 144\n",
    "\n",
    "### Discussion\n",
    "\n",
    "In our three trial runs we noticed that Value Iteration has fewer operations than policy iteration. This correlates with our hypothesis in the introduction. Interestingly enough, our other hypothesis that the number of counts in value iteration are roughly related to policy evaluation counts divided by the number of policy iterations. We got yields of 171, 93, and 135. Our value iteration internal counts were 144, 180 and 144.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "As shown in this lab, value iteration is more computationally efficient that policy iteration. This is largely due to the algorithms themselves, considering that policy evaluation is calculating the value function for a given policy. Value iteration calculates the value function for the optimum policy by maximizing over all possible actions for each state to derive it's value function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
