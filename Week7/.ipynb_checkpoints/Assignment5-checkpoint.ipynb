{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "### Introduction\n",
    "Up to this point in the course, we have been focused on calculating the value function for a determinstic grid world. The Monte Carlo aids in managing stochastic games, as it updates a value function based upon the resulting rewards seen for a set which contained that state. The algorithm works by following a policy to get an action for a specific state. For each run through of the policy, the value of each step is estimated in reverse order. For each state, the result is averaged with it's previous values to get the estimated value, and the value of that state is set to that average.\n",
    "\n",
    "Monte Carlo is a very simplistic algorithm, based on the idea that the value of a state is it's mean return. In the example in class, we utilized the first pass Monte Carlo version, which uses the first time a state appears in an episode to estimate the value of that state. There are two different implementations covered in this course, the simple Monte Carlo policy evaluation, and then a more complex policy iteration Monte Carlo. The policy iteration version updates the policies based on maximizing the value of the given state by choosing the highest reward + discount factor * V(s').\n",
    "\n",
    "To explore the Monte Carlo version, and highlight some of it's major flaws, I will refer to values from policy iteration for the calculated value function for the negative grid world. In this lab I will run the Monte Carlo policy iteration version 3 times at 100, 1000, and 10000 iterations and average the calculated values for each state. I will then compare this to the policy iteration value for each state. My hypothesis is that Monte Carlo will have a tendency to match the value function calculated as the number of episodes goes to infinity.\n",
    "\n",
    "### Results\n",
    "\n",
    "Policy Iteration Values:\n",
    " * 0.62| 0.80| 1.00| 0.00|\n",
    " * 0.46| 0.00| 0.80| 0.00|\n",
    " * 0.31| 0.46| 0.62| 0.46|\n",
    "\n",
    "Monte Carlo 100 Iteration Values:\n",
    " * 0.55| 0.75| 1.00| 0.00|\n",
    " * 0.37| 0.00| 0.29| 0.00|\n",
    " * 0.21| -.51| -.38| -.67|\n",
    "\n",
    "Monte Carlo 1000 Iteration Values:\n",
    " * 0.55| 0.77| 1.00| 0.00|\n",
    " * 0.36| 0.00| 0.74| 0.00|\n",
    " * 0.23| 0.14| -.11| 0.16|\n",
    " \n",
    "Monte Carlo 10000 Iteration Values:\n",
    " * 0.58| 0.78| 1.00| 0.00|\n",
    " * 0.41| 0.00| 0.77| 0.00|\n",
    " * 0.25| 0.19| 0.19| 0.15|\n",
    " \n",
    "### Discussion\n",
    "\n",
    "As one can see from the data above, as the number of episodes goes up, the values for the monte carlo value function become closer to those for of policy iteration. With the exception of the bottom right cell, improvement is shown in the gap at each cell for each higher number of iterations. Between trials of Monte Carlo runs, there are large standard deviations for the estimated value at the bottom right portion of the grid world. This is due to Monte Carlo's tendency to undersample the values which it thinks are too low. For this particular problem, Monte Carlo is much more expensive and has a lower accuracy until very high numbers of runs are performed.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Monte Carlo learning is a good introduction to reinforcement learning algorithms. It requires the process be finite and episodic, and tends to undersample portions of a sample space, as shown in our results for the bottom right portion of the grid world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
