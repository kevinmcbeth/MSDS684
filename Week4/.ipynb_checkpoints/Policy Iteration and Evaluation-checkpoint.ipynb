{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation and Policy Iteration\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Policy Iteration is a Reinforcement Learning Algorithm which utilizes Iterative Policy Evaluation to calcuate the value function under a given policy followed by a policy Improvement Step. These Policy Evaluation and Policy Improvement steps are conducted iteratively. Often the policy is randomized at the beginning prior to entering the iteration. At each iteration the policy is evaluated and then an attempt to find a better policy is made. If the policy is kept the same, then there is convergence and the optimal policy has been found.\n",
    "\n",
    "The purpose of this lab exercise is to experiment with the grid world problem (either a standard grid or a grid with a negative step cost). My goals for this assignment are to explore the algorithm in the following ways:\n",
    "\n",
    "1) Utilize the standard and negative grid and calculate the number of iterations necessary for convergence under different gamma conditions.\n",
    "2) Calculate the number of policy updates which occur at each iteration.\n",
    "3) Change the step cost to -1 to test if there is a different optimal policy for the negative grid.\n",
    "\n",
    "These goals are relatively simply accomplished. I expect to see that the negative grid takes longer to update than the standard grid, and I also expect to see that the number of policy updates occuring at each iteration goes down for both grid examples. I also expect higher gammas to have a longer time to convergence as it requires a longer look ahead. Theoretically, changing the grid world problem would allow us to show a different optimal policy under given conditions. Say if the step cost was -1, then entering the square with reward -1 might be more beneficial then continuing to play the game.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "#### Changing gammas for each grid and calculating iterations of evaluation and policy improvement:\n",
    "\n",
    "Standard Grid\n",
    "* 0.1 - 3, 4, 3, 3, 2\n",
    "* 0.5 - 2, 4, 2, 4, 3\n",
    "* 0.9 - 4, 3, 3, 5, 3\n",
    "\n",
    "Negative Grid\n",
    "* 0.1 - 5, 4, 3, 3, 4\n",
    "* 0.5 - 3, 3, 4, 5, 4\n",
    "* 0.9 - 5, 3, 3, 4, 5\n",
    "\n",
    "#### number of policy updates occuring at each iteration:\n",
    "\n",
    "Standard Grid:\n",
    "* 6, 3, 2, 0\n",
    "* 6, 3, 2, 1, 0\n",
    "* 6, 4, 3, 1, 1, 0\n",
    "\n",
    "Negative Grid:\n",
    "* 5, 3, 2, 2, 0\n",
    "* 8, 5, 2, 3, 1, 0\n",
    "* 6, 2, 3, 1, 0\n",
    "\n",
    "#### Changing the optimal policy found\n",
    "\n",
    "For standard problems, the optimal policy is:\n",
    "\n",
    "  R  |  R  |  R  |  X  |\n",
    "\n",
    "  U  |  X  |  U  |  X  |\n",
    "\n",
    "  U  |  R  |  U  |  L  |\n",
    "  \n",
    "In the altered grid world problem, with a step cost of -1, the optimal is:\n",
    "\n",
    "\n",
    "  R  |  R  |  R  |  X  |\n",
    "\n",
    "  U  |  X  |  U  |  X  |\n",
    "\n",
    "  U  |  R  |  U  |  U  |\n",
    "  \n",
    "## Explanation\n",
    "1) There was little impact of the difference on the number of iterations for the first experiment. Changing Gamma did not have a pronounced effect on the problem. There was a longer average convergence for the negative grid than the standard grid. This shows that as the problem becomes more complicated, the algorithm will take more iterations to solve and update.\n",
    "\n",
    "2) There is a trend that the number of policy updates at each step goes down over time. This is expected, as it is one of the criteria for convergence of the problem.\n",
    "\n",
    "3) The optimal policy is indeed related to the problem. Changing the step cost to -1 as opposed to -.1 for the standard negative grid world results in a different optimal policy.\n",
    "  \n",
    "## Conclusion\n",
    "This experiment begand to explore the grid world problem and the usage of the policy evaluation and iteration method of solving for the optimal policy. We calculated multiple times the number of loops of the algorithm to find convergence and showed than a negative grid is more complicated and thus takes slightly longer. We also retrieved the number of policy updates at each iteration and showed that they indeed go down over time, as expected. Finally, we solved for the optimal policy for a slightly different problem, and discovered that changing the parameters does indeed change the policy converged to.\n",
    "\n",
    "In the next study, we will compare Policy Iteration to Value Iteration. Value Iteration combines the policy evaluation and iteration into one algorithm, estimating the best policy at each point. This is brought up to explain the inefficiency in using policy iteration.\n",
    "\n",
    "Some follow up experiments for the policy iteration algorithm are:\n",
    "* Counting inner loop runs to estimate overall run time\n",
    "* Expanding the problem\n",
    "* Changing the rewards and evaluating the convergence for a more complicated grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
